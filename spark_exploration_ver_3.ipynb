{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, Tokenizer\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import  IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Text_Classification\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents read in is: 5820036\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/User/PycharmProjects/spark/animals_comments.csv'\n",
    "data = spark.read.csv([path])\n",
    "print(\"Number of documents read in is:\", data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|chan_owner   |id_comentator|text                                                                                                                   |\n",
      "+-------------+-------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|creator_name |userid       |comment                                                                                                                |\n",
      "|Doug The Pug | 87          |I shared this to my friends and mom the were lol                                                                       |\n",
      "|Doug The Pug | 87          |Super cute  üòÄüêïüê∂                                                                                                     |\n",
      "|bulletproof  | 530         |stop saying get em youre literally dumb . have some common sense or dont own this kind of dog. fucking retarded I swear|\n",
      "|Meu Zool√≥gico| 670         |Tenho uma jiboia e um largato                                                                                          |\n",
      "+-------------+-------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = data.withColumnRenamed('_c0', 'chan_owner').withColumnRenamed('_c1', 'id_comentator').withColumnRenamed('_c2', 'text')\n",
    "data.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|id_comentator|count|\n",
      "+-------------+-----+\n",
      "|      2036522| 2571|\n",
      "|       455571| 2159|\n",
      "|       569313| 1452|\n",
      "|      1727704| 1339|\n",
      "|      2041593| 1288|\n",
      "|      2288680| 1247|\n",
      "|       954873|  888|\n",
      "|       479268|  775|\n",
      "|      1568280|  757|\n",
      "|       575285|  742|\n",
      "|      2184324|  740|\n",
      "|       170012|  725|\n",
      "|       797741|  697|\n",
      "|      1367232|  671|\n",
      "|      1293328|  644|\n",
      "|      2056517|  642|\n",
      "|      1705977|  627|\n",
      "|      2071628|  605|\n",
      "|      1448539|  601|\n",
      "|       717385|  596|\n",
      "|      2427846|  596|\n",
      "|      2396540|  586|\n",
      "|       576447|  581|\n",
      "|       239250|  578|\n",
      "|      1766767|  573|\n",
      "|      1194456|  566|\n",
      "|         null|  565|\n",
      "|      2399121|  563|\n",
      "|       416516|  558|\n",
      "|      2042389|  544|\n",
      "|      2427873|  541|\n",
      "|      1810444|  541|\n",
      "|       620667|  538|\n",
      "|      2207685|  523|\n",
      "|      1377368|  517|\n",
      "|      2076074|  512|\n",
      "|      1288301|  509|\n",
      "|       635762|  508|\n",
      "|      2451022|  503|\n",
      "|      1007044|  498|\n",
      "|      1078410|  487|\n",
      "|       248498|  487|\n",
      "|       203589|  486|\n",
      "|      2492709|  482|\n",
      "|      1197060|  474|\n",
      "|      1554783|  473|\n",
      "|       852342|  472|\n",
      "|       333424|  471|\n",
      "|      1137039|  465|\n",
      "|      2255392|  464|\n",
      "+-------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"id_comentator\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(n=50, truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------+\n",
      "|id_comentator|concat_ws(, , collect_list(text))|\n",
      "+-------------+---------------------------------+\n",
      "|       100005|             Can you do blitz ...|\n",
      "|      1000095|             teor√≠a por dem√°s ...|\n",
      "|      1000928|             Life really took ...|\n",
      "|      1000955|             I love you Jungko...|\n",
      "|      1000967|             OK COOL  FISHES  ...|\n",
      "|      1001237|                            Quack|\n",
      "|      1001262|             la de la flor de ...|\n",
      "|      1001299|                               Hi|\n",
      "|        10013|              no quer√≠a caerse :C|\n",
      "|       100141|             Awwwww They Are S...|\n",
      "|      1001452|             I love these vlog...|\n",
      "|      1001603|             After watching vi...|\n",
      "|      1002165|                               D:|\n",
      "|      1002329|             HonestlyI LOVE th...|\n",
      "|      1002348|             1:20Well at Walgr...|\n",
      "|      1002567|                   quem √©  toddy?|\n",
      "|      1002904|             Omg sew that guy ...|\n",
      "|      1002952|             I can‚Äôt even do a...|\n",
      "|      1003021|             wtfÔºü durian is so...|\n",
      "|       100303|             Congrats!!!! You ...|\n",
      "+-------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF = data.groupby(\"id_comentator\").agg(f.concat_ws(\", \", f.collect_list(data.text)))\n",
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDistinctDF = countDistinctDF.withColumnRenamed('concat_ws(, , collect_list(text))', 'text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|word|  count|\n",
      "+----+-------+\n",
      "| the|2362893|\n",
      "|   I|2155003|\n",
      "|   a|1851088|\n",
      "| and|1705804|\n",
      "|  to|1697617|\n",
      "| you|1361504|\n",
      "|    |1239857|\n",
      "|  is|1033276|\n",
      "|  of| 911444|\n",
      "|  in| 744448|\n",
      "| for| 727594|\n",
      "|that| 726554|\n",
      "|  it| 659242|\n",
      "|  so| 648062|\n",
      "|  my| 621289|\n",
      "|your| 582033|\n",
      "|have| 560468|\n",
      "| are| 546201|\n",
      "|this| 521092|\n",
      "|like| 503676|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.withColumn('word', f.explode(f.split(f.col('text'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|    id_comentator|                text|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|            84058|               84058|\n",
      "|   mean|1269945.150110638|                null|\n",
      "| stddev|731914.3214415313|                null|\n",
      "|    min|          1000034| *KOVU* no!  remi...|\n",
      "|    max|           999990|üßÄüßÄ, Her editing...|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_list_cat = ['cat owner', 'cats owner', \"–∫–æ—Ç\", 'i have cat', 'my cat', 'cat', 'cats']\n",
    "\n",
    "cat = countDistinctDF.filter(f.col('text')\n",
    "                            .rlike('(^|\\s)(' + '|'\n",
    "                            .join(words_list_cat) + ')(\\s|$)'))\n",
    "\n",
    "cat.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|    id_comentator|                text|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|           143452|              143452|\n",
      "|   mean|1269379.064739425|                null|\n",
      "| stddev|733192.8865919772|                null|\n",
      "|    min|          1000014| *Thirsty* , i di...|\n",
      "|    max|           999976|üßü‚Äç‚ôÇÔ∏è I like turt...|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_list_dog = ['dog', 'dogs', \"—Å–æ–±–∞–∫–∞\", 'i have dog', 'dog owner', \"my dog\", \"mine dog\"]\n",
    "\n",
    "dog = countDistinctDF.filter(f.col('text')\n",
    "                            .rlike('(^|\\s)(' + '|'\n",
    "                            .join(words_list_dog) + ')(\\s|$)'))\n",
    "\n",
    "dog.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = dog.withColumn('category', lit(\"dog\"))\n",
    "cat = cat.withColumn('category', lit(\"cat\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog = dog.unionAll(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------+\n",
      "|id_comentator|                text|category|\n",
      "+-------------+--------------------+--------+\n",
      "|      1004717|hi taylor! I love...|     dog|\n",
      "|      1006412|My friends I see ...|     dog|\n",
      "|      1006679|Name him Bryan th...|     dog|\n",
      "|      1011010|People adopted hi...|     dog|\n",
      "|       101207|fuck that fish! b...|     dog|\n",
      "|      1025022|accent sounded en...|     dog|\n",
      "|       102640|My neighbor just ...|     dog|\n",
      "|      1026537|i looooove mice t...|     dog|\n",
      "|      1028988|I wish I was with...|     dog|\n",
      "|      1028999|I hope u are ok a...|     dog|\n",
      "|      1035275|Oh my God fucking...|     dog|\n",
      "|      1036191|Great love to see...|     dog|\n",
      "|      1044107|The way Coyote po...|     dog|\n",
      "|      1044800|I love gohan, I w...|     dog|\n",
      "|        10485|I wish all dog tr...|     dog|\n",
      "|      1049861|Nagini was the na...|     dog|\n",
      "|      1058469|Gone to the snow ...|     dog|\n",
      "|      1058822|Who needs TV when...|     dog|\n",
      "|      1059832|—É –º–µ–Ω—è –¥–æ–º–∞ —Å–æ–±–∞–∫...|     dog|\n",
      "|      1063787|women do this all...|     dog|\n",
      "|      1065080|all dogs are prot...|     dog|\n",
      "|       107104|Thats not an inti...|     dog|\n",
      "|      1074255|noo... noo üòÇüòÇüòÇ...|     dog|\n",
      "|      1074300|Spiders like Nope...|     dog|\n",
      "|      1078212|God bless you tho...|     dog|\n",
      "+-------------+--------------------+--------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_dog.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|id_comentator|                text|category|               words|            filtered|            features|label|\n",
      "+-------------+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|      1004717|hi taylor! I love...|     dog|[hi, taylor, i, l...|[hi, taylor, love...|(10000,[0,1,2,4,9...|  0.0|\n",
      "|      1006412|My friends I see ...|     dog|[my, friends, i, ...|[friends, see, no...|(10000,[2,4,14,16...|  0.0|\n",
      "|      1006679|Name him Bryan th...|     dog|[name, him, bryan...|[name, bryan, gat...|(10000,[2,26,95,3...|  0.0|\n",
      "|      1011010|People adopted hi...|     dog|[people, adopted,...|[people, adopted,...|(10000,[0,2,4,5,1...|  0.0|\n",
      "|       101207|fuck that fish! b...|     dog|[fuck, that, fish...|[fuck, fish, boil...|(10000,[2,6,113,1...|  0.0|\n",
      "+-------------+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(cat_dog)\n",
    "dataset = pipelineFit.transform(cat_dog)\n",
    "dataset.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 159361\n",
      "Test Dataset Count: 68149\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------+--------+------------------------------+-----+----------+\n",
      "|id_comentator|                          text|category|                   probability|label|prediction|\n",
      "+-------------+------------------------------+--------+------------------------------+-----+----------+\n",
      "|      2017407|Place human-dog friendship ...|     cat|  [1.0,3.8912470427199477E-25]|  1.0|       0.0|\n",
      "|       843061|The air shark was so cool!,...|     dog|   [1.0,2.712536412252654E-44]|  0.0|       0.0|\n",
      "|      1316036|French and English bulldogs...|     cat|[0.9999999999971767,2.82340...|  1.0|       0.0|\n",
      "|       636273|Hehe I thought that was dwa...|     dog|[0.9999999993466913,6.53308...|  0.0|       0.0|\n",
      "|      2302437|I love gohan, I love you Go...|     dog|[0.9999999958074084,4.19259...|  0.0|       0.0|\n",
      "|      2126410|Such a great video!  Thank ...|     cat|[0.9999999865016185,1.34983...|  1.0|       0.0|\n",
      "|       252989|Good morning brian üòÅWhat e...|     dog|[0.9999999515156569,4.84843...|  0.0|       0.0|\n",
      "|       776661|LOL  You guys are such lova...|     dog|[0.9999992231199026,7.76880...|  0.0|       0.0|\n",
      "|       776661|LOL  You guys are such lova...|     cat|[0.9999992231199026,7.76880...|  1.0|       0.0|\n",
      "|      1591150|It would always end like th...|     cat|[0.9999978285425059,2.17145...|  1.0|       0.0|\n",
      "+-------------+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20,\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"id_comentator\",\"text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5530393116905363"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.8722257300628284\n"
     ]
    }
   ],
   "source": [
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------+--------+---------------------------+-----+----------+\n",
      "|id_comentator|                          text|category|                probability|label|prediction|\n",
      "+-------------+------------------------------+--------+---------------------------+-----+----------+\n",
      "|      1718781|Dont Forget Subscribe VIRAL...|     dog|[1.0,1.015409805219536E-16]|  0.0|       0.0|\n",
      "|      1161308|Hello Brian!! Im on vacatio...|     dog|[1.0,9.973488263617031E-17]|  0.0|       0.0|\n",
      "|      1679441|You are my dog trainer and ...|     dog|[1.0,9.904250025978762E-17]|  0.0|       0.0|\n",
      "|      1044567|omg too cute in car while w...|     dog|[1.0,9.668966872316635E-17]|  0.0|       0.0|\n",
      "|      2170919|Rick please remember these ...|     cat|[1.0,9.301893088451775E-17]|  1.0|       0.0|\n",
      "|       229408|What kind of stupid people ...|     cat|[1.0,7.708844664600006E-17]|  1.0|       0.0|\n",
      "|       229408|What kind of stupid people ...|     dog|[1.0,7.708844664600006E-17]|  0.0|       0.0|\n",
      "|      2457135|As someone who has worked w...|     dog|[1.0,6.810467060248798E-17]|  0.0|       0.0|\n",
      "|       424912|I commented on how to get y...|     dog|[1.0,6.322208205195686E-17]|  0.0|       0.0|\n",
      "|       505289|What is wrong with hope for...|     dog|[1.0,6.297885443432076E-17]|  0.0|       0.0|\n",
      "+-------------+------------------------------+--------+---------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model_nb = nb.fit(trainingData)\n",
    "predictionsnb = model_nb.transform(testData)\n",
    "predictionsnb.filter(predictionsnb['prediction'] == 0) \\\n",
    "    .select(\"id_comentator\",\"text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8789944410998412"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictionsnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------+--------+------------------------------+-----+----------+\n",
      "|id_comentator|                          text|category|                   probability|label|prediction|\n",
      "+-------------+------------------------------+--------+------------------------------+-----+----------+\n",
      "|         8460|A bullmastiff is a form of ...|     dog|[0.7439669895522577,0.25603...|  0.0|       0.0|\n",
      "|      2230282|I have a beautiful male Rid...|     dog|[0.7245044172289337,0.27549...|  0.0|       0.0|\n",
      "|       602073|You know I like how this vi...|     dog|[0.7234606942531768,0.27653...|  0.0|       0.0|\n",
      "|      1599417|All of these breeds are alr...|     dog|[0.7231087793344182,0.27689...|  0.0|       0.0|\n",
      "|      2487142|dam that mutherfucker bit t...|     dog|[0.7220139497321232,0.27798...|  0.0|       0.0|\n",
      "|      1599417|Awesome video.  This is wit...|     dog|[0.7206074195630413,0.27939...|  0.0|       0.0|\n",
      "|      2230282|Not a bad list but I must s...|     dog|[0.719923818013677,0.280076...|  0.0|       0.0|\n",
      "|       406237|Nobody should own a Pitbull...|     dog|[0.7189054852034218,0.28109...|  0.0|       0.0|\n",
      "|       443305|The media has done an amazi...|     dog|[0.716239844223714,0.283760...|  0.0|       0.0|\n",
      "|      1407409|Im so angry at all the bigo...|     dog|[0.7137382847914966,0.28626...|  0.0|       0.0|\n",
      "+-------------+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf = rfModel.transform(testData)\n",
    "predictions_rf.filter(predictions_rf['prediction'] == 0) \\\n",
    "    .select(\"id_comentator\",\"text\",\"category\",\"probability\",\"label\",\"prediction\")\\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4599925919799822"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|id_comentator|                text|               words|            filtered|            features|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       100005|Can you do blitz ...|[can, you, do, bl...|[blitz, bull, 125...|(10000,[879,5932]...|\n",
      "|      1000095|teor√≠a por dem√°s ...|[teor, a, por, de...|[teor, por, dem, ...|(10000,[29,58,60,...|\n",
      "|      1000928|Life really took ...|[life, really, to...|[life, really, to...|(10000,[16,36,80,...|\n",
      "|      1000955|I love you Jungko...|[i, love, you, ju...|[love, jungkook, ...|(10000,[0,29,121]...|\n",
      "|      1000967|OK COOL  FISHES  ...|[ok, cool, fishes...|[ok, cool, fishes...|(10000,[1,3,7,10,...|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "pipelineFit = pipeline.fit(countDistinctDF)\n",
    "dataset = pipelineFit.transform(countDistinctDF)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all = model_nb.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|id_comentator|                text|               words|            filtered|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       100005|Can you do blitz ...|[can, you, do, bl...|[blitz, bull, 125...|(10000,[879,5932]...|[-20.271947072500...|[0.56649480294087...|       0.0|\n",
      "|      1000095|teor√≠a por dem√°s ...|[teor, a, por, de...|[teor, por, dem, ...|(10000,[29,58,60,...|[-313.22484819330...|[0.80878949064078...|       0.0|\n",
      "|      1000928|Life really took ...|[life, really, to...|[life, really, to...|(10000,[16,36,80,...|[-81.935000407094...|[0.78860546667384...|       0.0|\n",
      "|      1000955|I love you Jungko...|[i, love, you, ju...|[love, jungkook, ...|(10000,[0,29,121]...|[-17.157900844375...|[0.58077857243690...|       0.0|\n",
      "|      1000967|OK COOL  FISHES  ...|[ok, cool, fishes...|[ok, cool, fishes...|(10000,[1,3,7,10,...|[-489.20946939208...|[0.11185363076587...|       1.0|\n",
      "|      1001237|               Quack|             [quack]|             [quack]|       (10000,[],[])|[-0.4604996672841...|[0.63096829251457...|       0.0|\n",
      "|      1001262|la de la flor de ...|[la, de, la, flor...|[la, de, la, flor...|(10000,[26,29,54,...|[-278.45134814910...|[0.69403503198477...|       0.0|\n",
      "|      1001299|                  Hi|                [hi]|                [hi]| (10000,[101],[1.0])|[-6.9127826713980...|[0.63689031775101...|       0.0|\n",
      "|        10013| no quer√≠a caerse :C|[no, quer, a, cae...|   [quer, caerse, c]|(10000,[79,6848],...|[-18.025571147624...|[0.69080910296475...|       0.0|\n",
      "|       100141|Awwwww They Are S...|[awwwww, they, ar...|[awwwww, sooooo, ...|(10000,[8,1119,15...|[-23.962093377951...|[0.46606934877206...|       1.0|\n",
      "|      1001452|I love these vlog...|[i, love, these, ...|[love, vlogs, ama...|(10000,[0,4,17,22...|[-120.62502734747...|[0.54431190244347...|       0.0|\n",
      "|      1001603|After watching vi...|[after, watching,...|[watching, videos...|(10000,[5,13,25,3...|[-234.69976135611...|[0.89793673261840...|       0.0|\n",
      "|      1002165|                  D:|                 [d]|                 [d]| (10000,[102],[1.0])|[-6.8919231346446...|[0.64542575564250...|       0.0|\n",
      "|      1002329|HonestlyI LOVE th...|[honestlyi, love,...|[honestlyi, love,...|(10000,[0,13,22,4...|[-103.43891979900...|[0.71773668584468...|       0.0|\n",
      "|      1002348|1:20Well at Walgr...|[1, 20well, at, w...|[1, 20well, walgr...|  (10000,[67],[1.0])|[-6.6025454575824...|[0.64902725248518...|       0.0|\n",
      "|      1002567|      quem √©  toddy?|       [quem, toddy]|       [quem, toddy]|(10000,[8869],[1.0])|[-12.390766558096...|[0.50316624396803...|       0.0|\n",
      "|      1002904|Omg sew that guy ...|[omg, sew, that, ...|[omg, sew, guy, s...|(10000,[0,1,5,8,1...|[-355.15246218017...|[0.90343099948666...|       0.0|\n",
      "|      1002952|I can‚Äôt even do a...|[i, can, t, even,...|[even, finger, pr...|(10000,[57,1444,6...|[-27.262181956529...|[0.52988973178340...|       0.0|\n",
      "|      1003021|wtfÔºü durian is so...|[wtf, durian, is,...|[wtf, durian, fki...|(10000,[498,602,1...|[-26.030278370657...|[0.49856710929309...|       1.0|\n",
      "|       100303|Congrats!!!! You ...|[congrats, you, d...|[congrats, deserv...|(10000,[0,13,17,7...|[-66.387828113828...|[0.75685576496090...|       0.0|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 - dog, 1 - cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(chan_owner='creator_name', id_comentator='userid', text='comment'),\n",
       " Row(chan_owner='Doug The Pug', id_comentator=' 87', text='I shared this to my friends and mom the were lol'),\n",
       " Row(chan_owner='Doug The Pug', id_comentator=' 87', text='Super cute  üòÄüêïüê∂')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_comentator=' 100005', text='Can you do blitz bull 125 wd', words=['can', 'you', 'do', 'blitz', 'bull', '125', 'wd'], filtered=['blitz', 'bull', '125', 'wd'], features=SparseVector(10000, {879: 1.0, 5932: 1.0}), rawPrediction=DenseVector([-20.2719, -20.5395]), probability=DenseVector([0.5665, 0.4335]), prediction=0.0),\n",
       " Row(id_comentator=' 1000095', text='teor√≠a por dem√°s absurda recordar que el reactor arc que Tony Stark tenia en su pecho le fue retirado en Iron Man 3 en Los Vengadores el cetro de Loki no hizo efecto en Tony porque t√©cnicamente no toc√≥ su pecho hasta un ni√±o de 5 a√±os entiende eso', words=['teor', 'a', 'por', 'dem', 's', 'absurda', 'recordar', 'que', 'el', 'reactor', 'arc', 'que', 'tony', 'stark', 'tenia', 'en', 'su', 'pecho', 'le', 'fue', 'retirado', 'en', 'iron', 'man', '3', 'en', 'los', 'vengadores', 'el', 'cetro', 'de', 'loki', 'no', 'hizo', 'efecto', 'en', 'tony', 'porque', 't', 'cnicamente', 'no', 'toc', 'su', 'pecho', 'hasta', 'un', 'ni', 'o', 'de', '5', 'a', 'os', 'entiende', 'eso'], filtered=['teor', 'por', 'dem', 'absurda', 'recordar', 'que', 'el', 'reactor', 'arc', 'que', 'tony', 'stark', 'tenia', 'en', 'su', 'pecho', 'le', 'fue', 'retirado', 'en', 'iron', 'man', '3', 'en', 'los', 'vengadores', 'el', 'cetro', 'de', 'loki', 'hizo', 'efecto', 'en', 'tony', 'porque', 'cnicamente', 'toc', 'su', 'pecho', 'hasta', 'un', 'ni', 'o', 'de', '5', 'os', 'entiende', 'eso'], features=SparseVector(10000, {29: 1.0, 58: 2.0, 60: 1.0, 63: 2.0, 99: 2.0, 106: 1.0, 153: 1.0, 164: 4.0, 222: 1.0, 232: 1.0, 307: 1.0, 582: 1.0, 670: 1.0, 744: 2.0, 1111: 1.0, 1184: 1.0, 1402: 1.0, 1538: 1.0, 1613: 2.0, 1750: 1.0, 1788: 1.0, 1805: 1.0, 2341: 1.0, 3150: 1.0, 3230: 1.0, 3999: 1.0, 4533: 1.0, 5306: 1.0, 6340: 1.0, 8419: 1.0}), rawPrediction=DenseVector([-313.2248, -314.667]), probability=DenseVector([0.8088, 0.1912]), prediction=0.0),\n",
       " Row(id_comentator=' 1000928', text='Life really took a massive dump on you didnt it Toby. I hope things have gotten better.', words=['life', 'really', 'took', 'a', 'massive', 'dump', 'on', 'you', 'didnt', 'it', 'toby', 'i', 'hope', 'things', 'have', 'gotten', 'better'], filtered=['life', 'really', 'took', 'massive', 'dump', 'didnt', 'toby', 'hope', 'things', 'gotten', 'better'], features=SparseVector(10000, {16: 1.0, 36: 1.0, 80: 1.0, 85: 1.0, 117: 1.0, 127: 1.0, 402: 1.0, 610: 1.0, 1232: 1.0, 2058: 1.0, 3148: 1.0}), rawPrediction=DenseVector([-81.935, -83.2515]), probability=DenseVector([0.7886, 0.2114]), prediction=0.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='', words=[], filtered=[], features=SparseVector(10000, {}), rawPrediction=DenseVector([-0.4605, -0.9969]), probability=DenseVector([0.631, 0.369]), prediction=0.0, chan_owner='Mermaid Melissa', id_comentator=' 2527324', text=None),\n",
       " Row(text='', words=[], filtered=[], features=SparseVector(10000, {}), rawPrediction=DenseVector([-0.4605, -0.9969]), probability=DenseVector([0.631, 0.369]), prediction=0.0, chan_owner='Brave Wilderness', id_comentator=' 439837', text=None),\n",
       " Row(text='', words=[], filtered=[], features=SparseVector(10000, {}), rawPrediction=DenseVector([-0.4605, -0.9969]), probability=DenseVector([0.631, 0.369]), prediction=0.0, chan_owner='ViralBe', id_comentator=' 558547', text=None),\n",
       " Row(text=' Main Aisa Kyun Hoon one of best Bollywood video song. Please react to this.', words=['main', 'aisa', 'kyun', 'hoon', 'one', 'of', 'best', 'bollywood', 'video', 'song', 'please', 'react', 'to', 'this'], filtered=['main', 'aisa', 'kyun', 'hoon', 'one', 'best', 'bollywood', 'video', 'song', 'please', 'react'], features=SparseVector(10000, {3: 1.0, 4: 1.0, 33: 1.0, 48: 1.0, 349: 1.0, 1211: 1.0, 1447: 1.0}), rawPrediction=DenseVector([-48.595, -47.3662]), probability=DenseVector([0.2264, 0.7736]), prediction=1.0, chan_owner='Alex Dauterive', id_comentator=' 1918156', text=' Main Aisa Kyun Hoon one of best Bollywood video song. Please react to this.'),\n",
       " Row(text=' We Should Probably Wrap This Up . No No No! Gotta Get An Outro.  Coyote Is Legendary', words=['we', 'should', 'probably', 'wrap', 'this', 'up', 'no', 'no', 'no', 'gotta', 'get', 'an', 'outro', 'coyote', 'is', 'legendary'], filtered=['probably', 'wrap', 'gotta', 'get', 'outro', 'coyote', 'legendary'], features=SparseVector(10000, {5: 1.0, 18: 1.0, 244: 1.0, 706: 1.0, 2931: 1.0, 3842: 1.0, 5818: 1.0}), rawPrediction=DenseVector([-57.8712, -59.2983]), probability=DenseVector([0.8064, 0.1936]), prediction=0.0, chan_owner='Brave Wilderness', id_comentator=' 557789', text=' We Should Probably Wrap This Up . No No No! Gotta Get An Outro.  Coyote Is Legendary')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = predictions_all.join(data, predictions_all.id_comentator == data.id_comentator, 'inner').drop(data.id_comentator)\n",
    "x = pred_data.dropDuplicates([\"text\", \"id_comentator\"])\n",
    "x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-4e30ec65313d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtop100\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtop100\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"z\")\n",
    "cv = cv_tmp.fit(x)\n",
    "\n",
    "top100 = list(cv.vocabulary[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|id_comentator|                text|               words|            filtered|     words_for_count|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       100005|Can you do blitz ...|[can, you, do, bl...|       [blitz, bull]|       [blitz, bull]|\n",
      "|      1000095|teor√≠a por dem√°s ...|[teor, a, por, de...|[teor, absurda, r...|[teor, absurda, r...|\n",
      "|      1000928|Life really took ...|[life, really, to...|[life, really, to...|[life, took, mass...|\n",
      "|      1000955|I love you Jungko...|[i, love, you, ju...|          [jungkook]|          [jungkook]|\n",
      "|      1000967|OK COOL  FISHES  ...|[ok, cool, fishes...|[cool, fishes, ha...|[cool, fishes, ha...|\n",
      "|      1001237|               Quack|             [quack]|             [quack]|             [quack]|\n",
      "|      1001262|la de la flor de ...|[la, de, la, flor...|[flor, hojas, tam...|[flor, hojas, tam...|\n",
      "|      1001299|                  Hi|                [hi]|                  []|                  []|\n",
      "|        10013| no quer√≠a caerse :C|[no, quer, a, cae...|      [quer, caerse]|      [quer, caerse]|\n",
      "|       100141|Awwwww They Are S...|[awwwww, they, ar...|[awwwww, sooooo, ...|    [awwwww, sooooo]|\n",
      "|      1001452|I love these vlog...|[i, love, these, ...|[these, vlogs, am...|[vlogs, positive,...|\n",
      "|      1001603|After watching vi...|[after, watching,...|[after, watching,...|[after, watching,...|\n",
      "|      1002165|                  D:|                 [d]|                  []|                  []|\n",
      "|      1002329|HonestlyI LOVE th...|[honestlyi, love,...|[honestlyi, chann...|[honestlyi, chann...|\n",
      "|      1002348|1:20Well at Walgr...|[1, 20well, at, w...| [20well, walgreens]| [20well, walgreens]|\n",
      "|      1002567|      quem √©  toddy?|       [quem, toddy]|       [quem, toddy]|       [quem, toddy]|\n",
      "|      1002904|Omg sew that guy ...|[omg, sew, that, ...|[money, cats, oth...|[money, cats, oth...|\n",
      "|      1002952|I can‚Äôt even do a...|[i, can, t, even,...|[even, finger, pr...|[even, finger, pr...|\n",
      "|      1003021|wtfÔºü durian is so...|[wtf, durian, is,...|[durian, fking, d...|[durian, fking, d...|\n",
      "|       100303|Congrats!!!! You ...|[congrats, you, d...|[congrats, deserv...|[congrats, deserv...|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "more_then_3_charachters = [word for word in cv.vocabulary if len(word) <= 3]\n",
    "contains_digits = [word for word in cv.vocabulary if any(char.isdigit() for char in word)]\n",
    "stopwords = [] \n",
    "stopwords = stopwords + top100 + more_then_3_charachters + contains_digits\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_for_count\", stopWords = stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='', words=[], filtered=[], features=SparseVector(10000, {}), rawPrediction=DenseVector([-0.4605, -0.9969]), probability=DenseVector([0.631, 0.369]), prediction=0.0, chan_owner='Mermaid Melissa', id_comentator=' 2527324', text=None, words_for_count=[]),\n",
       " Row(text='', words=[], filtered=[], features=SparseVector(10000, {}), rawPrediction=DenseVector([-0.4605, -0.9969]), probability=DenseVector([0.631, 0.369]), prediction=0.0, chan_owner='Brave Wilderness', id_comentator=' 439837', text=None, words_for_count=[]),\n",
       " Row(text='', words=[], filtered=[], features=SparseVector(10000, {}), rawPrediction=DenseVector([-0.4605, -0.9969]), probability=DenseVector([0.631, 0.369]), prediction=0.0, chan_owner='ViralBe', id_comentator=' 558547', text=None, words_for_count=[])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = remover.transform(x)\n",
    "x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|     words_for_count| count|\n",
      "+--------------------+------+\n",
      "|                  []|421066|\n",
      "|              [cool]|  5259|\n",
      "|             [first]|  4218|\n",
      "|            [coyote]|  3662|\n",
      "|              [want]|  2571|\n",
      "|              [asmr]|  2358|\n",
      "|             [funny]|  2092|\n",
      "|              [keep]|  2076|\n",
      "|[looking, back, t...|  2015|\n",
      "|          [adorable]|  1858|\n",
      "|              [fake]|  1734|\n",
      "|            [crying]|  1616|\n",
      "|[fabulous, zapped...|  1553|\n",
      "|          [birthday]|  1529|\n",
      "|              [dogs]|  1525|\n",
      "|             [bless]|  1514|\n",
      "|    [raptor, fossil]|  1511|\n",
      "|              [made]|  1464|\n",
      "|[scarlet, johanss...|  1452|\n",
      "|             [crazy]|  1440|\n",
      "|             [thats]|  1422|\n",
      "|              [guys]|  1414|\n",
      "|             [hello]|  1315|\n",
      "|    [coyote, statue]|  1305|\n",
      "|[theres, dogs, di...|  1287|\n",
      "|              [awww]|  1285|\n",
      "|              [best]|  1283|\n",
      "|[nope, still, eve...|  1246|\n",
      "|      [raptor, bone]|  1226|\n",
      "|             [gohan]|  1212|\n",
      "|             [sweet]|  1157|\n",
      "|[careful, human, ...|  1130|\n",
      "|              [cats]|  1112|\n",
      "|             [early]|  1090|\n",
      "|           [animals]|  1077|\n",
      "|           [channel]|  1068|\n",
      "|              [song]|  1062|\n",
      "|             [brave]|  1052|\n",
      "|            [monkey]|  1038|\n",
      "|             [cried]|   980|\n",
      "|             [sorry]|   962|\n",
      "|              [vids]|   932|\n",
      "|             [awwww]|   907|\n",
      "|              [feel]|   894|\n",
      "|              [clip]|   892|\n",
      "|[winters, here, b...|   888|\n",
      "|              [mark]|   886|\n",
      "|              [make]|   883|\n",
      "|              [fuck]|   848|\n",
      "|              [poor]|   846|\n",
      "+--------------------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.groupBy('words_for_count')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.na.drop(subset=[\"words_for_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------------------+------------------+--------------------+\n",
      "|summary|    text|prediction|          chan_owner|     id_comentator|                text|\n",
      "+-------+--------+----------+--------------------+------------------+--------------------+\n",
      "|  count| 4182775|   4182775|             4161182|           4182775|             4182336|\n",
      "|   mean|Infinity|       0.0|1.018439742824547...|1270007.2489434045|            Infinity|\n",
      "| stddev|     NaN|       0.0|1.005104405673807...| 733881.6607035523|                 NaN|\n",
      "|    min|        |       0.0|#CameraLord‚Ñ¢ ‚Ä¢ Ko...|                 1|                 ...|\n",
      "|    max|      üß°|       0.0|üêæ Life Is Better...|            userid|üß°üß°üíõüß°üíõüß°üíõüß°?...|\n",
      "+-------+--------+----------+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog_owners = x.filter(x[\"prediction\"] == 0)\n",
    "dog_owners.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+------------------+--------------------+\n",
      "|summary|                text|prediction|          chan_owner|     id_comentator|                text|\n",
      "+-------+--------------------+----------+--------------------+------------------+--------------------+\n",
      "|  count|             1564885|   1564885|             1554894|           1564885|             1564855|\n",
      "|   mean|   274.5550769230769|       1.0|1.835060233047752...|1269655.4293401751|1.285245763906350...|\n",
      "| stddev|  408.42268784588794|       0.0|1.348430557917316E13| 733556.5605698468|3.995521631830355...|\n",
      "|    min| *BUY MY MERCH* ,...|       1.0|#CameraLord‚Ñ¢ ‚Ä¢ Ko...|                10|                    |\n",
      "|    max|üßü‚Äç‚ôÇÔ∏è I like turt...|       1.0|üêæ Life Is Better...|            999997|üßü‚Äç‚ôÇÔ∏è I like turt...|\n",
      "+-------+--------------------+----------+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_owners = x.filter(x[\"prediction\"] == 1)\n",
    "cat_owners.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_owners.groupBy('words_for_count')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_owners.groupBy('words_for_count')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x.groupBy().avg(\"prediction\").take(1)[0][0]\n",
    "x = x.withColumn(\"mean_category\", lit(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
